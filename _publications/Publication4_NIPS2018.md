---
title: "Rethinking Layer-wise Feature Amounts in Convolutional Neural Network Architectures"
collection: publications
permalink: /publication/4/
excerpt: 'We characterize convolutional neural networks with respect to the relative amount of features per layer. Using a skew normal distribution as a parametrized framework, we investigate the common assumption of monotonously increasing feature-counts with higher layers of architecture designs. Our evaluation on models with VGG-type layers on the MNIST, Fashion-MNIST and CIFAR-10 image classification benchmarks provides evidence that motivates rethinking of our common assumption: architectures that favor larger early layers seem to yield better accuracy.'
date: 2018-12-07
venue: 'NeurIPS CRACT 2018, Canada'
paperurl: 'https://arxiv.org/abs/1812.05836'

citation: 'Martin Mundt, <b>Sagnik Majumder</b>, Tobias Weis, Visvanathan Ramesh, &quot;Rethinking Layer-wise Feature Amounts in Convolutional Neural Network Architectures&quot; In: International Conference on Neural Information Processing Systems (NeurIPS) 2018, Critiquing and Correcting Trends in Machine Learning (CRACT) Workshop.
'

#  <i>Published in NeurIPS CRACT 2018, Montreal, Canada</i>.
---
Abstract: We characterize convolutional neural networks with respect to the relative amount
of features per layer. Using a skew normal distribution as a parametrized framework,
we investigate the common assumption of monotonously increasing feature-counts
with higher layers of architecture designs. Our evaluation on models with VGG-
type layers on the MNIST, Fashion-MNIST and CIFAR-10 image classification
benchmarks provides evidence that motivates rethinking of our common assump-
tion: architectures that favor larger early layers seem to yield better accuracy.

[Download paper here](https://arxiv.org/pdf/1812.05836.pdf)