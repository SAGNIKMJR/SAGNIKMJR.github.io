---
title: "Audio-Visual Waypoints for Navigation"
collection: publications
permalink: /publication/9/
excerpt: "In audio-visual navigation, an agent intelligently travels through a complex, unmapped 3D environment using both sights and sounds to find a sound source (e.g.,a phone ringing in another room). Existing models learn to act at a fixed granularity of agent motion and rely on simple recurrent aggregations of the audio observations. We introduce a reinforcement learning approach to audio-visual navigation with two key novel elements 1) audio-visual waypoints that are dynamically set and learned end-to-end within the navigation policy, and 2) an acoustic memory that provides a structured, spatially grounded record of what the agent has heard as it moves. Both new ideas capitalize on the synergy of audio and visual data for revealing the geometry of an unmapped space. We demonstrate our approach on the challenging Replica environments of real-world 3D scenes. Our model improves the state of the art by a substantial margin, and our experiments reveal that learning the links between sights, sounds, and space is essential for audio-visual navigation."
<!-- date: TBD -->
venue: '<b>TBD</b>'
paperurl: 'https://arxiv.org/abs/2008.09622'
citation: 'Changan Chen, <b>Sagnik Majumder</b>, Ziad Al-Halah, Ruohan Gao, Santhosh K. Ramakrishnan, Kristen
Grauman. &quot;Audio-Visual Waypoints for Navigation&quot; In: arXiv.
'

#  <i>Under submission</i>.
---
Abstract: In audio-visual navigation, an agent intelligently travels through a complex, un-mapped 3D environment using both sights and sounds to find a sound source (e.g.,a phone ringing in another room). Existing models learn to act at a fixed granularityof agent motion and rely on simple recurrent aggregations of the audio observations.We introduce a reinforcement learning approach to audio-visual navigation withtwo key novel elements 1) audio-visual waypoints that are dynamically set andlearned end-to-end within the navigation policy, and 2) an acoustic memory thatprovides a structured, spatially grounded record of what the agent has heard asit moves.  Both new ideas capitalize on the synergy of audio and visual data forrevealing the geometry of an unmapped space. We demonstrate our approach on thechallenging Replica environments of real-world 3D scenes. Our model improvesthe state of the art by a substantial margin, and our experiments reveal that learningthe links between sights, sounds, and space is essential for audio-visual navigation.

<!-- [Download paper here](http://openaccess.thecvf.com/content_CVPR_2019/papers/Mundt_Meta-Learning_Convolutional_Neural_Architectures_for_Multi-Target_Concrete_Defect_Classification_With_CVPR_2019_paper.pdf) -->